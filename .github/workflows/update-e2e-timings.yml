name: Update E2E Timings

on:
  workflow_dispatch:
    inputs:
      days_back:
        description: "Number of days to look back for timing artifacts"
        type: number
        default: 7
  push:
    branches:
      - master
      - generate-new-test-timings
    paths:
      - ".github/workflows/update-e2e-timings.yml"
  schedule:
    # Run weekly on Sundays at 6 AM UTC
    - cron: "0 6 * * 0"

jobs:
  collect-and-average-timings:
    runs-on: ubuntu-22.04
    timeout-minutes: 30
    steps:
      - uses: actions/checkout@v4
        with:
          token: ${{ secrets.GITHUB_TOKEN }}

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: 22

      - name: Collect recent timing artifacts
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          DAYS_BACK: ${{ inputs.days_back || 7 }}
        run: |
          echo "Searching for merged timing artifacts from the last $DAYS_BACK days..."

          # Calculate cutoff date
          CUTOFF_DATE=$(date -d "$DAYS_BACK days ago" -u +"%Y-%m-%dT%H:%M:%SZ")
          echo "Looking for artifacts created after: $CUTOFF_DATE"

          # Find all merged timing artifacts
          mkdir -p timing-artifacts
          echo "Querying GitHub API for merged timing artifacts..."

          # Query for merged timing artifacts
          gh api repos/${{ github.repository }}/actions/artifacts \
            --jq "[.artifacts[] | select(.name | startswith(\"merged-e2e-timings-\")) | select(.created_at > \"$CUTOFF_DATE\") | select(.expired == false) | {name, created_at, archive_download_url}]" \
            > timing-artifacts/artifact-list.json
            
          echo "Found artifacts:"
          cat timing-artifacts/artifact-list.json

          # Check if we have any artifacts
          if [ ! -s timing-artifacts/artifact-list.json ]; then
            echo "‚ùå No timing artifacts found in the specified time range"
            echo "This might be because:"
            echo "  - No e2e test runs have completed in the last $DAYS_BACK days"
            echo "  - No merged timing artifacts were generated"
            echo "  - The artifacts have expired (GitHub expires artifacts after 90 days by default)"
            exit 1
          fi

          # Try to count artifacts, handling both array and single object cases
          TOTAL_ARTIFACTS=$(jq -r 'if type == "array" then length else 1 end' timing-artifacts/artifact-list.json 2>/dev/null || echo "0")
          echo "Found $TOTAL_ARTIFACTS artifact(s) to process"

          if [ "$TOTAL_ARTIFACTS" -eq "0" ]; then
            echo "‚ùå No valid timing artifacts found"
            exit 1
          fi

          # Download each artifact
          ARTIFACT_COUNT=0

          # Handle both single object and array cases
          if [ "$TOTAL_ARTIFACTS" -eq "1" ]; then
            # Single artifact - read directly
            NAME=$(jq -r '.name' timing-artifacts/artifact-list.json)
            URL=$(jq -r '.archive_download_url' timing-artifacts/artifact-list.json)
            CREATED=$(jq -r '.created_at' timing-artifacts/artifact-list.json)
            
            echo "Downloading artifact: $NAME (created: $CREATED)"
            curl -L -H "Authorization: token ${{ secrets.GITHUB_TOKEN }}" "$URL" -o "timing-artifacts/${NAME}.zip"
            
            # Extract the timing file
            mkdir -p "timing-artifacts/${NAME}"
            unzip -q "timing-artifacts/${NAME}.zip" -d "timing-artifacts/${NAME}/"
            
            # Check if timing file exists
            if [ -f "timing-artifacts/${NAME}/e2e/support/timings.json" ]; then
              cp "timing-artifacts/${NAME}/e2e/support/timings.json" "timing-artifacts/${NAME}-timings.json"
              ARTIFACT_COUNT=$((ARTIFACT_COUNT + 1))
              echo "‚úì Extracted timing data from $NAME"
            else
              echo "‚ö† No timing data found in $NAME"
            fi
            
            # Cleanup extracted directory
            rm -rf "timing-artifacts/${NAME}"
            rm "timing-artifacts/${NAME}.zip"
          else
            # Multiple artifacts - iterate through array
            while read -r artifact; do
              if [ -n "$artifact" ]; then
                NAME=$(echo "$artifact" | jq -r '.name')
                URL=$(echo "$artifact" | jq -r '.archive_download_url')
                CREATED=$(echo "$artifact" | jq -r '.created_at')
                
                echo "Downloading artifact: $NAME (created: $CREATED)"
                curl -L -H "Authorization: token ${{ secrets.GITHUB_TOKEN }}" "$URL" -o "timing-artifacts/${NAME}.zip"
                
                # Extract the timing file
                mkdir -p "timing-artifacts/${NAME}"
                unzip -q "timing-artifacts/${NAME}.zip" -d "timing-artifacts/${NAME}/"
                
                # Check if timing file exists
                if [ -f "timing-artifacts/${NAME}/e2e/support/timings.json" ]; then
                  cp "timing-artifacts/${NAME}/e2e/support/timings.json" "timing-artifacts/${NAME}-timings.json"
                  ARTIFACT_COUNT=$((ARTIFACT_COUNT + 1))
                  echo "‚úì Extracted timing data from $NAME"
                else
                  echo "‚ö† No timing data found in $NAME"
                fi
                
                # Cleanup extracted directory
                rm -rf "timing-artifacts/${NAME}"
                rm "timing-artifacts/${NAME}.zip"
              fi
            done < <(jq -c '.[]' timing-artifacts/artifact-list.json)
          fi

          echo "Successfully collected $ARTIFACT_COUNT timing artifacts"
          ls -la timing-artifacts/

      - name: Average timing data
        run: |
          node -e "
          const fs = require('fs');
          const path = require('path');

          console.log('üîÑ Processing timing artifacts to calculate averages...');

          const artifactDir = 'timing-artifacts';
          const timingFiles = fs.readdirSync(artifactDir).filter(f => f.endsWith('-timings.json'));

          console.log(\`Found \${timingFiles.length} timing files:\`);
          timingFiles.forEach(f => console.log(\`  - \${f}\`));

          if (timingFiles.length === 0) {
            console.log('‚ùå No timing files found');
            process.exit(1);
          }

          // Collect all timing data per spec
          const specTimings = {};
          let totalFiles = 0;

          timingFiles.forEach(file => {
            try {
              const content = fs.readFileSync(path.join(artifactDir, file), 'utf8');
              const data = JSON.parse(content);
              
              if (data.durations && Array.isArray(data.durations)) {
                totalFiles++;
                data.durations.forEach(item => {
                  if (item.spec && typeof item.duration === 'number') {
                    if (!specTimings[item.spec]) {
                      specTimings[item.spec] = [];
                    }
                    specTimings[item.spec].push(item.duration);
                  }
                });
              }
            } catch (error) {
              console.log(\`‚ö† Error reading \${file}: \${error.message}\`);
            }
          });

          console.log(\`\nProcessed \${totalFiles} timing files\`);
          console.log(\`Found timing data for \${Object.keys(specTimings).length} unique specs\`);

          // Calculate simple averages per spec
          const averagedTimings = { durations: [] };

          Object.entries(specTimings).forEach(([spec, durations]) => {
            // Simple average - include all specs regardless of sample count
            const average = durations.reduce((sum, d) => sum + d, 0) / durations.length;
            averagedTimings.durations.push({
              spec: spec,
              duration: Math.round(average)
            });
            console.log(\`‚úì \${spec}: \${durations.length} samples, avg \${Math.round(average)}ms\`);
          });

          console.log(\`\nüìä Results:\`);
          console.log(\`  ‚úì Processed \${Object.keys(specTimings).length} specs\`);

          // Sort by spec name for consistency
          averagedTimings.durations.sort((a, b) => a.spec.localeCompare(b.spec));

          // Write averaged timings
          fs.writeFileSync('averaged-timings.json', JSON.stringify(averagedTimings, null, 2));
          console.log('‚úÖ Averaged timings written to averaged-timings.json');
          "

      - name: Set up git configuration
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "41898282+github-actions[bot]@users.noreply.github.com"

      - name: Update timing file
        run: |
          # Replace the timing file with averaged data
          cp averaged-timings.json e2e/support/timings.json
          echo "‚úÖ Updated e2e/support/timings.json with averaged timing data"

      - name: Create Pull Request
        uses: peter-evans/create-pull-request@v6
        with:
          token: ${{ secrets.GITHUB_TOKEN }}
          branch: update-e2e-timings-$(date +%Y%m%d-%H%M%S)
          base: master
          commit-message: |
            Update E2E test timings with averaged data

            Collected timing data from ${{ inputs.days_back || 7 }} days of test runs.
            All specs with timing data were included (simple averages).

            ü§ñ Generated automatically by update-e2e-timings workflow

            Co-Authored-By: GitHub Actions <actions@github.com>
          committer: "github-actions[bot] <41898282+github-actions[bot]@users.noreply.github.com>"
          author: "github-actions[bot] <41898282+github-actions[bot]@users.noreply.github.com>"
          title: "üìä Update E2E test timings with recent data"
          body: |
            ## Summary
            This PR updates the E2E test timing data with averaged values from recent test runs.

            ## Details
            - üìÖ Collected data from the last **${{ inputs.days_back || 7 }} days** of test runs

            ## Generated automatically
            ü§ñ This PR was created automatically by the `update-e2e-timings` workflow.
            The timing data helps cypress-split optimize test distribution across parallel jobs.

            Co-Authored-By: GitHub Actions <actions@github.com>
          add-paths: e2e/support/timings.json
